{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import pexpect\n",
    "import time\n",
    "slim = tf.contrib.slim\n",
    "patches = mpl.patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, sess, batch_iterators, num_layers = 1, num_hidden_nodes = 30, activation_fn = tf.nn.relu,\n",
    "                 learning_rate = 0.00001, model_name = 'NN', target_scaling = True, feature_scaling = True,\n",
    "                 checkpoint_dir = 'checkpoint', quantile = False):\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.train_iter = batch_iterators['train']\n",
    "        self.val_iter = batch_iterators['val']\n",
    "        self.test_iter = batch_iterators['test']\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.learning_rate = learning_rate\n",
    "                \n",
    "        self.targets_dim = self.train_iter.targets_dim\n",
    "        self.features_dim = self.train_iter.features_dim\n",
    "        \n",
    "        self.target_scaling = target_scaling\n",
    "        self.feature_scaling = feature_scaling\n",
    "        \n",
    "        self.quantile = quantile\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "         # Scalers\n",
    "        self.t_scaler, self.rev_t_scaler, self.rev_var_scaler = add_scaler(self.train_iter.target_mean,\n",
    "                                    self.train_iter.target_std, scaling = self.target_scaling, with_var = True, name = 'targets')\n",
    "        self.f_scaler, _ = add_scaler(self.train_iter.feature_mean, self.train_iter.feature_std, scaling = self.feature_scaling, name = 'features')\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "       \n",
    "    def build_model(self):\n",
    "        self.targets_pl = tf.placeholder(tf.float32, [None, self.targets_dim], 'targets_pl')\n",
    "        self.features_pl = tf.placeholder(tf.float32, [None, self.features_dim], 'features_pl')\n",
    "        \n",
    "        # Scaling step\n",
    "        self.targets = self.t_scaler(self.targets_pl)\n",
    "        self.features = self.f_scaler(self.features_pl)\n",
    "        \n",
    "        # Build the mean prediction network\n",
    "        hidden = slim.stack(self.features, slim.fully_connected, [self.num_hidden_nodes] * self.num_layers, \n",
    "                       activation_fn=self.activation_fn)\n",
    "        \n",
    "        self.output = slim.fully_connected(hidden, self.targets_dim, activation_fn=None, scope = 'final_layer')\n",
    "        self.sc_output = self.rev_t_scaler(self.output)\n",
    "        \n",
    "        # Loss        \n",
    "        if self.quantile:\n",
    "            self.loss = quantile_loss(self.targets, self.output, self.quantile)\n",
    "        else:\n",
    "            self.loss = tf.reduce_mean(tf.pow(self.targets - self.output, 2))\n",
    "                  \n",
    "        # Optimizer\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def train(self, viz_every = 500, num_steps = 5000):\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "        for step in xrange(num_steps):\n",
    "            \n",
    "            t_batch, f_batch = self.train_iter.next_batch()\n",
    "            \n",
    "            # Initially just train the mean prediction network \n",
    "            _ = self.sess.run(self.opt, feed_dict = {self.features_pl: f_batch, self.targets_pl: t_batch})\n",
    "\n",
    "            if step % viz_every == 0:\n",
    "\n",
    "                _, TRAIN_LOSS = self.run_data_set(self.train_iter)\n",
    "                _, VAL_LOSS = self.run_data_set(self.val_iter)\n",
    "                _, TEST_LOSS = self.run_data_set(self.test_iter)\n",
    "\n",
    "                print \"Step: {0}, Train Loss: {1:.2f}, Val Loss: {2:.2f}, Test Loss: {3:.2f}\".format(step,\n",
    "                                                                    TRAIN_LOSS, VAL_LOSS, TEST_LOSS)            \n",
    "\n",
    "    \n",
    "                if VAL_LOSS < best_val_loss:\n",
    "                    self.save()\n",
    "                    best_val_loss = VAL_LOSS\n",
    "\n",
    "        self.saver.restore(self.sess, self.checkpoint_dir + '/' + self.model_name)\n",
    "        \n",
    "        self.TRAIN_PREDS, TRAIN_LOSS = self.run_data_set(self.train_iter)\n",
    "        self.VAL_PREDS, VAL_LOSS = self.run_data_set(self.val_iter)\n",
    "        self.TEST_PREDS, TEST_LOSS = self.run_data_set(self.test_iter)\n",
    "                \n",
    "        print \"Final Losses, Train: {1:.2f}, Val: {2:.2f}, Test: {3:.2f}\".format(step,\n",
    "                                                                            TRAIN_LOSS, VAL_LOSS, TEST_LOSS) \n",
    "                \n",
    "    def run_data_set(self, iterator):\n",
    "        \n",
    "        # Store starting value of iterator to return to\n",
    "        counter_start = iterator.counter\n",
    "        # Make sure we start from the first batch\n",
    "        iterator.counter = 0\n",
    "\n",
    "        preds_list = []\n",
    "        loss_list = []\n",
    "        \n",
    "        for step in xrange(iterator.num_batches):\n",
    "            \n",
    "            t_batch, f_batch = iterator.next_batch()\n",
    "            PREDS, LOSS = self.sess.run([self.sc_output, self.loss], feed_dict = {self.features_pl: f_batch, self.targets_pl: t_batch})\n",
    "            preds_list.append(PREDS)\n",
    "            loss_list.append(LOSS)\n",
    "            \n",
    "        loss = np.average(loss_list)\n",
    "        preds = np.concatenate(preds_list, axis = 0)\n",
    "\n",
    "        # Return iterator counter to starting value\n",
    "        iterator.counter = counter_start\n",
    "        \n",
    "        return preds, loss        \n",
    "    \n",
    "    def predict(self, features):\n",
    "        \n",
    "        self.saver.restore(self.sess, self.checkpoint_dir + '/' + self.model_name)\n",
    "        \n",
    "        PREDS = self.sess.run(self.sc_output, feed_dict = {self.features_pl: features})\n",
    "        \n",
    "        return PREDS\n",
    "\n",
    "    def residuals(self):\n",
    "        train_res = np.concatenate(self.train_iter.targets_data, axis = 0) - self.TRAIN_PREDS\n",
    "        val_res = np.concatenate(self.val_iter.targets_data, axis = 0) - self.VAL_PREDS\n",
    "        test_res = np.concatenate(self.test_iter.targets_data, axis = 0) - self.TEST_PREDS\n",
    "        return train_res, val_res, test_res\n",
    "   \n",
    "    def save(self):\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        self.saver.save(self.sess, self.checkpoint_dir + '/' + self.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_scaler(mean, std, scaling = False, with_var = False, name = 'Scaler'):\n",
    "    \n",
    "    mean = mean.astype(np.float32)\n",
    "    std = std.astype(np.float32)\n",
    "    \n",
    "    if scaling:\n",
    "    \n",
    "        def scaler(data):\n",
    "            scaled_data = tf.divide((data - mean), std)\n",
    "            return scaled_data\n",
    "        def reverse_scaler(data):\n",
    "            reversed_data = tf.add(tf.multiply(data,std), mean, name = name + '_reverse')\n",
    "            return reversed_data\n",
    "        def reverse_var_scaler(data):\n",
    "            reversed_data = tf.multiply(data, tf.pow(std,2))\n",
    "            return reversed_data\n",
    "        \n",
    "    else:  \n",
    "        def scaler(data):\n",
    "            return data   \n",
    "        def reverse_scaler(data):\n",
    "            return data  \n",
    "        def reverse_var_scaler(data):\n",
    "            return data\n",
    "\n",
    "    if with_var:\n",
    "        return scaler, reverse_scaler, reverse_var_scaler\n",
    "    else:\n",
    "        return scaler, reverse_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_UCI_data(loc, shuffle = True):\n",
    "    data = np.array(pd.read_csv(loc, header = None))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "    targets = data[:,-1]\n",
    "    features = data[:,0:-1]\n",
    "    return targets, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class monitor_gpu(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.command = 'nvidia-smi --query-gpu=utilization.gpu --format=csv -l 1 -f ./temp_gpu_log.csv'\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        self.p = pexpect.spawn(self.command)\n",
    "        \n",
    "    def stop_monitoring(self):\n",
    "        self.p.sendcontrol('c')\n",
    "        time.sleep(1)\n",
    "        df = pd.read_csv('temp_gpu_log.csv', sep = ' ')\n",
    "        self.usage = df['utilization.gpu'].iloc[2:-1]\n",
    "        self.average_use = np.average(self.usage)\n",
    "        \n",
    "        os.remove('temp_gpu_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_blog_graph():\n",
    "    fig = plt.figure(figsize = [6,4])\n",
    "    ax = plt.axes()    \n",
    "    mpl.rc('axes', labelsize = 12)\n",
    "    mpl.rc('figure', titlesize = 14)\n",
    "    return fig, ax\n",
    "\n",
    "colours = {'orange': '#f78500', 'yellow': '#fed16c', 'green': '#139277', 'blue': '#0072df',\n",
    "               'dark_blue': '#001e78', 'pink': '#fd6d77'}\n",
    "\n",
    "def show_blog_colours():    \n",
    "    num_colors = len(colours.keys())\n",
    "\n",
    "    fig = plt.figure(figsize = [num_colors * 2, 2])\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    for c in range(num_colors):\n",
    "        color = colours[colours.keys()[c]]\n",
    "        ax.add_patch(patches.Rectangle((0.03 + c*0.12 + c*0.03, 0.1), 0.12, 0.6, color = color))\n",
    "        ax.annotate(colours.keys()[c], xy=(0.07 + 0.145 * c, 0.85))\n",
    "    _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_sorter(targets_data, features_data = None, batch_size = 100, shuffle_train = False, random_draw = False,\n",
    "                train_ratio = 0.7, val_ratio = 0.15, test_ratio = 0.15, set_batch_size = False):\n",
    "\n",
    "            \n",
    "    assert train_ratio + val_ratio + test_ratio == 1, 'Percentages don\\'t add up for the data sets'\n",
    "    if features_data:\n",
    "        assert len(targets_data) == len(features_data), 'Targets and features data different sizes'\n",
    "        \n",
    "    # Get rid of excess data if need a constant batch size\n",
    "    if set_batch_size:\n",
    "        if (len(targets_data)%batch_size) != 0:\n",
    "            targets_data = targets_data[:-(len(targets_data)%batch_size)]\n",
    "            if features_data:\n",
    "                features_data = features_data[:len(targets_data)]\n",
    "\n",
    "    # Split the data into train, val and test sets\n",
    "    train_size = int(ceil(train_ratio * len(targets_data)))\n",
    "    val_size = int(ceil(val_ratio * len(targets_data)))\n",
    "    train_size = int(batch_size * round(float(train_size)/batch_size))\n",
    "    val_size = int(batch_size * round(float(val_size)/batch_size))\n",
    "    test_size = int(len(targets_data) - train_size - val_size)\n",
    "    \n",
    "    if random_draw:\n",
    "        train_indices = random.sample(range(train_size + val_size), train_size)\n",
    "        val_indices = [i for i in range(train_size + val_size) if i not in train_indices]\n",
    "        train_t_data = [targets_data[i] for i in train_indices]\n",
    "        val_t_data = [targets_data[i] for i in val_indices]\n",
    "        if features_data:\n",
    "            train_f_data = [features_data[i] for i in train_indices]\n",
    "            val_f_data = [features_data[i] for i in val_indices]\n",
    "    else:\n",
    "        train_t_data = targets_data[0: train_size]\n",
    "        val_t_data = targets_data[train_size: train_size + val_size]\n",
    "        if features_data:\n",
    "            train_f_data = features_data[0: train_size]\n",
    "            val_f_data = features_data[train_size: train_size + val_size]\n",
    "    \n",
    "    test_t_data = targets_data[train_size + val_size :]\n",
    "    if features_data:    \n",
    "        test_f_data = features_data[train_size + val_size :]\n",
    "    \n",
    "    print \"Train data: {} observations\".format(len(train_t_data))\n",
    "    print \"Val data: {} observations\".format(len(val_t_data))\n",
    "    print \"Test data: {} observations\\n\".format(len(test_t_data))\n",
    "\n",
    "\n",
    "    # Shuffle train data if required\n",
    "    if shuffle_train == True:\n",
    "\n",
    "        indices = np.random.permutation(len(train_t_data))\n",
    "        train_t_data = [train_t_data[i] for i in indices]\n",
    "        \n",
    "        if features_data:\n",
    "            train_f_data = [train_f_data[i] for i in indices]    \n",
    "    \n",
    "    # Create the iterators\n",
    "    if features_data:\n",
    "        train_iter = batch_iterator(train_t_data, train_f_data, batch_size)\n",
    "        val_iter = batch_iterator(val_t_data, val_f_data, batch_size)\n",
    "        test_iter = batch_iterator(test_t_data, test_f_data, batch_size)\n",
    "    else:\n",
    "        train_iter = batch_iterator(train_t_data, batch_size = batch_size)\n",
    "        val_iter = batch_iterator(val_t_data, batch_size = batch_size)\n",
    "        test_iter = batch_iterator(test_t_data, batch_size = batch_size)\n",
    "        \n",
    "\n",
    "    iter_dict = {'train': train_iter, 'val': val_iter, 'test': test_iter}\n",
    "    \n",
    "    return iter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_batch_sorter(targets_data, features_data = None, batch_size = 100, shuffle = False, random_draw = False,\n",
    "                train_ratio = 0.7, val_ratio = 0.15, test_ratio = 0.15, set_batch_size = False, num_bootstraps = 100):\n",
    "\n",
    "    \n",
    "    # features_data: should be shape [data_size * features_dim]\n",
    "    # targets_data: should be shape [data_size * targets_dim]\n",
    "    \n",
    "    assert train_ratio + val_ratio + test_ratio == 1, 'Percentages don\\'t add up for the data sets'\n",
    "    assert targets_data.shape[0] == features_data.shape[0], 'Targets and features data different sizes'\n",
    "\n",
    "    data_size = targets_data.shape[0]\n",
    "       \n",
    "    # Expand out to at least 2 dims if data is of shape (data_size,)\n",
    "    if len(targets_data.shape) == 1:\n",
    "        targets_data = np.expand_dims(targets_data, axis = 1)\n",
    "    if len(features_data.shape) == 1:\n",
    "        features_data = np.expand_dims(features_data, axis = 1)\n",
    "    \n",
    "    # Get rid of excess data if need a constant batch size\n",
    "    if set_batch_size:\n",
    "        if (data_size%batch_size) != 0:\n",
    "            targets_data = targets_data[:-(data_size%batch_size), :]\n",
    "            features_data = features_data[:len(targets_data), :]\n",
    "                \n",
    "    # Shuffle data if required - this will give a random test set\n",
    "    if shuffle == True:\n",
    "        indices = np.random.permutation(len(targets_data))\n",
    "        targets_data = targets_data[indices, :]\n",
    "        features_data = features_data[indices, :]\n",
    "\n",
    "    # Split the data into train, val and test sets\n",
    "    train_size = int(ceil(train_ratio * data_size))\n",
    "    val_size = int(ceil(val_ratio * data_size))\n",
    "    train_size = int(batch_size * round(float(train_size)/batch_size))\n",
    "    val_size = int(batch_size * round(float(val_size)/batch_size))\n",
    "    test_size = int(data_size - train_size - val_size)\n",
    "        \n",
    "    # Draw the bootstrap val and train sets\n",
    "    train_t_list = []\n",
    "    val_t_list = []\n",
    "    \n",
    "    train_f_list = []\n",
    "    val_f_list = []\n",
    "    \n",
    "    for b in range(num_bootstraps):\n",
    "    \n",
    "        train_indices = random.sample(range(train_size + val_size), train_size)\n",
    "        val_indices = [i for i in range(train_size + val_size) if i not in train_indices]\n",
    "        train_t_list.append(targets_data[train_indices, :])\n",
    "        val_t_list.append(targets_data[val_indices, :])\n",
    "        train_f_list.append(features_data[train_indices, :])\n",
    "        val_f_list.append(features_data[val_indices, :])  \n",
    "    \n",
    "    # Concatenate the boostraps together to give one matrix of size [num_bootstraps * train/val_size * num_features/targets]\n",
    "    \n",
    "    train_t_data = np.concatenate([np.expand_dims(b, axis = 0) for b in train_t_list], axis = 0)\n",
    "    train_f_data = np.concatenate([np.expand_dims(b, axis = 0) for b in train_f_list], axis = 0)\n",
    "    \n",
    "    val_t_data = np.concatenate([np.expand_dims(b, axis = 0) for b in val_t_list], axis = 0)\n",
    "    val_f_data = np.concatenate([np.expand_dims(b, axis = 0) for b in val_f_list], axis = 0)\n",
    "    \n",
    "    # Test data is the same for each bootstrap\n",
    "    test_t_data = targets_data[train_size + val_size :, :]\n",
    "    test_f_data = features_data[train_size + val_size :, :]\n",
    "    \n",
    "    test_t_data = np.tile(np.expand_dims(test_t_data, axis = 0), [num_bootstraps, 1, 1])\n",
    "    test_f_data = np.tile(np.expand_dims(test_f_data, axis = 0), [num_bootstraps, 1, 1])\n",
    "\n",
    "    print \"Train data: {} observations\".format(train_size)\n",
    "    print \"Val data: {} observations\".format(val_size)\n",
    "    print \"Test data: {} observations\\n\".format(test_size)\n",
    "    \n",
    "    # Create the iterators\n",
    "    train_iter = bootstrap_batch_iterator(train_t_data, train_f_data, batch_size)\n",
    "    val_iter = bootstrap_batch_iterator(val_t_data, val_f_data, batch_size)\n",
    "    test_iter = bootstrap_batch_iterator(test_t_data, test_f_data, batch_size)\n",
    "\n",
    "    iter_dict = {'train': train_iter, 'val': val_iter, 'test': test_iter,} #'target_rescale': rev_target_scale}\n",
    "                            \n",
    "    return iter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class batch_iterator(object):\n",
    "    \n",
    "    def __init__(self, targets_data, features_data = None, batch_size = 50, shuffle = False):\n",
    "               \n",
    "        targets_data = self.sort_format(targets_data)\n",
    "        self.num_data_points = len(targets_data)\n",
    "        if features_data:\n",
    "            features_data = self.sort_format(features_data)\n",
    "        else:\n",
    "            self.features_data = None\n",
    "        \n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(len(targets_data))\n",
    "            targets_data = [targets_data[i] for i in indices]\n",
    "            if features_data:\n",
    "                features_data = [features_data[i] for i in indices]    \n",
    "\n",
    "        self.targets_data = targets_data\n",
    "        if features_data:\n",
    "            self.features_data = features_data\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.counter = 0\n",
    "        self.num_batches = int(ceil(float(len(targets_data))/float(batch_size)))\n",
    "        \n",
    "        self.targets_dim = targets_data[0].shape[1]\n",
    "        if features_data:\n",
    "            self.features_dim = features_data[0].shape[1]\n",
    "        \n",
    "        self.target_mean, self.target_std = self.target_scale_factors()\n",
    "        if features_data:\n",
    "            self.feature_mean, self.feature_std = self.feature_scale_factors()\n",
    "\n",
    "    def next_batch(self):\n",
    "        \n",
    "        targets_batch = self.new_batch(self.counter, self.targets_data)\n",
    "        if self.features_data:\n",
    "            features_batch = self.new_batch(self.counter, self.features_data)\n",
    "        \n",
    "        self.counter += 1\n",
    "        \n",
    "        if self.counter == self.num_batches:\n",
    "            self.counter = 0\n",
    "        \n",
    "        if self.features_data:\n",
    "            return targets_batch, features_batch\n",
    "        else:\n",
    "            return targets_batch\n",
    "        \n",
    "    def new_batch(self, counter, data):\n",
    "        try:\n",
    "            new_batch = data[counter*self.batch_size: (counter*self.batch_size) + self.batch_size]\n",
    "        except:\n",
    "            new_batch = data[counter*self.batch_size: ]\n",
    "            \n",
    "        new_batch = np.concatenate(new_batch, axis = 0)\n",
    "        \n",
    "        return new_batch\n",
    "    \n",
    "    def sort_format(self, data):\n",
    "        if type(data[0]) != np.ndarray:\n",
    "            data = [np.array(t) for t in data]\n",
    "        if len(data[0].shape) == 0:\n",
    "            data = [np.expand_dims(t,0) for t in data]\n",
    "        if len(data[0].shape) == 1:\n",
    "            data = [np.expand_dims(t,0) for t in data]\n",
    "        return data\n",
    "    \n",
    "    def target_scale_factors(self):\n",
    "        joined = np.concatenate(self.targets_data, axis = 0)\n",
    "        mean = np.average(self.targets_data, axis = 0)\n",
    "        std = np.std(self.targets_data, axis = 0)\n",
    "        return mean, std\n",
    "    \n",
    "    def feature_scale_factors(self):\n",
    "        joined = np.concatenate(self.features_data, axis = 0)\n",
    "        mean = np.average(self.features_data, axis = 0)\n",
    "        std = np.std(self.features_data, axis = 0)\n",
    "        return mean, std\n",
    "    \n",
    "    def sample_targets(self, sample_size):\n",
    "        \n",
    "        sample = random.sample(self.targets_data, sample_size)\n",
    "        sample = np.concatenate(sample, axis = 0)\n",
    "        return sample\n",
    "    \n",
    "    def sample_features(self, sample_size, index = 'random'):\n",
    "        # Return the same set of features tiled, so can draw from distribution for the next prediction\n",
    "        # Can define which features to return using the index input if required.\n",
    "        if index == 'random':\n",
    "            index = random.randint(0, len(self.features_data)-1)\n",
    "            \n",
    "        sample = self.features_data[index]\n",
    "        sample = np.tile(sample, [sample_size, 1])\n",
    "        \n",
    "      \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bootstrap_batch_iterator(object):\n",
    "    \n",
    "    def __init__(self, targets_data, features_data = None, batch_size = 50, shuffle = False):\n",
    "        \n",
    "        # Data will be of shape [num_bootstrap * data_size * targets/features_dim]\n",
    "        \n",
    "        self.num_data_points = targets_data.shape[1]\n",
    "        self.targets_data = targets_data\n",
    "        self.features_data = features_data\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.counter = 0\n",
    "        self.num_batches = int(ceil(float(self.num_data_points/float(batch_size))))\n",
    "        \n",
    "        self.targets_dim = targets_data.shape[2]\n",
    "        self.features_dim = features_data.shape[2]\n",
    "        \n",
    "        self.target_mean, self.target_std = self.scale_factors(targets_data)\n",
    "        self.feature_mean, self.feature_std = self.scale_factors(features_data)\n",
    "\n",
    "    \n",
    "    def next_batch(self):\n",
    "        \n",
    "        targets_batch = self.new_batch(self.counter, self.targets_data)\n",
    "        features_batch = self.new_batch(self.counter, self.features_data)\n",
    "        \n",
    "        self.counter += 1\n",
    "        \n",
    "        if self.counter == self.num_batches:\n",
    "            self.counter = 0\n",
    "            \n",
    "        return targets_batch, features_batch\n",
    "        \n",
    "    def new_batch(self, counter, data):\n",
    "        try:\n",
    "            new_batch = data[:, counter*self.batch_size: (counter*self.batch_size) + self.batch_size, :]\n",
    "        except:\n",
    "            new_batch = data[:, counter*self.batch_size:, :]\n",
    "                    \n",
    "        return new_batch\n",
    "    \n",
    "    def scale_factors(self, data):\n",
    "        mean = np.average(data[0,:,:], axis = 0)\n",
    "        std = np.std(data[0,:,:], axis = 0)\n",
    "        return mean, std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
