{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from IPython import display\n",
    "\n",
    "from blog_functions import add_scaler, monitor_gpu, read_UCI_data, bootstrap_batch_sorter, NN, batch_sorter, colours, show_blog_colours, single_blog_graph \n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_targets, housing_features = read_UCI_data(loc = '/home/joseph/Documents/Data/UCI_Data/housing.csv', shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bootstrap_NN(object):\n",
    "    \n",
    "    def __init__(self, sess, batch_iterators, num_layers = 1, num_hidden_nodes = 30, activation_fn = tf.nn.relu,\n",
    "                 learning_rate = 0.00001, model_name = 'NN', target_scaling = True, feature_scaling = True,\n",
    "                 checkpoint_dir = 'checkpoint_bootstrap', num_bootstraps= 10):\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.train_iter = batch_iterators['train']\n",
    "        self.val_iter = batch_iterators['val']\n",
    "        self.test_iter = batch_iterators['test']\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.learning_rate = learning_rate * num_bootstraps\n",
    "                \n",
    "        self.targets_dim = self.train_iter.targets_dim\n",
    "        self.features_dim = self.train_iter.features_dim\n",
    "        \n",
    "        self.target_scaling = target_scaling\n",
    "        self.feature_scaling = feature_scaling\n",
    "                \n",
    "        self.num_bootstraps = num_bootstraps\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        self.best_step = [0] * self.num_bootstraps\n",
    "        self.best_val_losses = [float('inf')] * self.num_bootstraps\n",
    "    \n",
    "        \n",
    "         # Scalers - values are scaled by (value - mean)/stdev, where mean and stdev are calculated on the train dataset\n",
    "        self.t_scaler, self.rev_t_scaler = add_scaler(self.train_iter.target_mean, self.train_iter.target_std,\n",
    "                                                      scaling = self.target_scaling, name = 'targets')\n",
    "        self.f_scaler, _ = add_scaler(self.train_iter.feature_mean, self.train_iter.feature_std, \n",
    "                                      scaling = self.feature_scaling, name = 'features')\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        self.saver = tf.train.Saver(max_to_keep = None)\n",
    "        \n",
    "       \n",
    "    def build_model(self):\n",
    "        self.targets_pl = tf.placeholder(tf.float32, [self.num_bootstraps, None, self.targets_dim], 'targets_pl')\n",
    "        self.features_pl = tf.placeholder(tf.float32, [self.num_bootstraps, None, self.features_dim], 'features_pl')\n",
    "        \n",
    "        # Scaling step\n",
    "        self.targets = self.t_scaler(self.targets_pl)\n",
    "        self.features = self.f_scaler(self.features_pl)        \n",
    "\n",
    "        layer_list = []\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            \n",
    "            if layer == 0:\n",
    "                input_size = self.features_dim\n",
    "                input_matrix = self.features\n",
    "            else:\n",
    "                input_size = self.num_hidden_nodes\n",
    "                input_matrix = layer_list[layer - 1]\n",
    "            \n",
    "            weights = tf.Variable(tf.truncated_normal([self.num_bootstraps, input_size, self.num_hidden_nodes], stddev = 0.1), name = 'weights_layer_' + str(layer))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape = [self.num_bootstraps, 1, self.num_hidden_nodes]), name = 'bias_layer_' + str(layer))\n",
    "                        \n",
    "            layer_inner = tf.matmul(input_matrix, weights) + bias\n",
    "                    \n",
    "            layer_list.append(self.activation_fn(layer_inner))\n",
    "\n",
    "        # Output layer and losses\n",
    "        output_weights = tf.Variable(tf.truncated_normal([self.num_bootstraps, self.num_hidden_nodes, self.targets_dim], stddev = 0.1), name = 'weights_final_layer')\n",
    "        output_bias = tf.Variable(tf.constant(0.1, shape = [self.num_bootstraps,1, self.targets_dim]), name = 'bias_final_layer')\n",
    "                      \n",
    "        self.output = tf.matmul(layer_list[self.num_layers - 1], output_weights) + output_bias\n",
    "        # Rescale the outputs of the network (if we've scaled the targets before training)\n",
    "        self.sc_output = self.rev_t_scaler(self.output)\n",
    "                    \n",
    "        # Create separate loss functions - these will not be used for training as faster with one loss op, but will\n",
    "        # be used at eval stage\n",
    "        \n",
    "        self.loss_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            self.loss_list.append(tf.reduce_mean(tf.pow(self.targets[b,:,:] - self.output[b,:,:], 2), name = 'loss_' + str(b)))\n",
    "        \n",
    "        # Main loss op used for training\n",
    "        self.loss = tf.reduce_mean(tf.pow(self.targets - self.output, 2), name = 'loss')\n",
    "                       \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        # Trainable vars\n",
    "        self.trainable_vars = tf.trainable_variables()\n",
    "\n",
    "    def train(self, viz_every = 500, num_steps = 5000):\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "        for step in xrange(num_steps):\n",
    "            \n",
    "            t_batch, f_batch = self.train_iter.next_batch()\n",
    "            \n",
    "            # Initially just train the mean prediction network \n",
    "            _ = self.sess.run(self.optimizer, feed_dict = {self.features_pl: f_batch, \n",
    "                                    self.targets_pl: t_batch})\n",
    "\n",
    "            if step % viz_every == 0:\n",
    "                \n",
    "                _, TRAIN_LOSSES = self.run_data_set(self.train_iter)\n",
    "                _, VAL_LOSSES = self.run_data_set(self.train_iter)\n",
    "                                \n",
    "                print \"Step: {0}, Train Loss: {1:.2f}, Val Loss: {2:.2f}\".format(step,\n",
    "                                        np.average(TRAIN_LOSSES), np.average(VAL_LOSSES))    \n",
    "                \n",
    "                save_update = False\n",
    "                \n",
    "                # Update the best val losses if any of the networks beat their best val loss so far\n",
    "                for b in range(self.num_bootstraps):\n",
    "                    if VAL_LOSSES[b] < self.best_val_losses[b]:\n",
    "                        self.best_val_losses[b] = VAL_LOSSES[b]\n",
    "                        self.best_step[b] = step\n",
    "                        save_update = True\n",
    "                \n",
    "                # Save if any of the separate bootstrapped networks have improved on their best val loss\n",
    "                if save_update:                    \n",
    "                    T_VARS = self.sess.run(self.trainable_vars)\n",
    "                    self.update_t_vars(T_VARS, step = step)\n",
    "                    \n",
    "        self.restore_best_vars()\n",
    "                \n",
    "        self.TRAIN_PREDS, self.TRAIN_LOSSES = self.run_data_set(self.train_iter)\n",
    "        self.VAL_PREDS, self.VAL_LOSSES = self.run_data_set(self.train_iter)\n",
    "        self.TEST_PREDS, self.TEST_LOSSES = self.run_data_set(self.test_iter)\n",
    "                \n",
    "        print \"Final Losses, Train: {1:.2f}, Val: {2:.2f}, Test: {3:.2f}\".format(step,\n",
    "                                            np.average(self.TRAIN_LOSSES), np.average(self.VAL_LOSSES), np.average(self.TEST_LOSSES)) \n",
    "        \n",
    "                    \n",
    "    def run_data_set(self, iterator):\n",
    "        \"\"\"\n",
    "        Runs predictions and loss ops for the whole data set stored in \"iterator\"\n",
    "        \"\"\"\n",
    "        # Store starting value of iterator to return to\n",
    "        counter_start = iterator.counter\n",
    "        # Make sure we start from the first batch\n",
    "        iterator.counter = 0\n",
    "        \n",
    "        preds_dict = {}\n",
    "        loss_dict = {}\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            preds_dict[b] = []\n",
    "            loss_dict[b] = []\n",
    "        \n",
    "        for step in xrange(iterator.num_batches):\n",
    "            \n",
    "            t_batch, f_batch = iterator.next_batch()\n",
    "            \n",
    "            OUTPUT = self.sess.run([self.sc_output] + self.loss_list, feed_dict = {self.features_pl: f_batch, \n",
    "                                                    self.targets_pl: t_batch})\n",
    "                 \n",
    "            LOSSES = OUTPUT[1:]\n",
    "            PREDS = OUTPUT[0]\n",
    "            \n",
    "            for b in range(self.num_bootstraps):\n",
    "                preds_dict[b].append(PREDS[b, :, :])\n",
    "                loss_dict[b].append(LOSSES[b])\n",
    "                \n",
    "        all_preds_list = []\n",
    "        average_loss_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            all_preds_list.append(np.concatenate(preds_dict[b], axis = 0))\n",
    "            average_loss_list.append(np.average(loss_dict[b]))\n",
    "        # Return iterator counter to starting value\n",
    "        iterator.counter = counter_start\n",
    "        \n",
    "        return all_preds_list, average_loss_list          \n",
    "        \n",
    "\n",
    "    \n",
    "    def update_t_vars(self, current_vars, step):\n",
    "        \"\"\"\n",
    "        Creates or updates the numpy versions of the variables, if the best val loss has been beaten for an individual\n",
    "        network\n",
    "        \"\"\"   \n",
    "        if not hasattr(self, 'best_vars'):            \n",
    "            self.best_vars = [np.zeros_like(v) for v in current_vars]\n",
    "        \n",
    "        # If loss was lowest for this bootstrap with these variables, update our new best variables\n",
    "        for b in range(self.num_bootstraps):\n",
    "            if self.best_step[b] == step:\n",
    "                # Assign the correct parts of these variables to our new var list\n",
    "                for num, v in enumerate(self.best_vars):\n",
    "                    v[b] = current_vars[num][b]      \n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Returns the predictions of each of the individual networks in a list, evaluated at the provided \"features\"\n",
    "        \"\"\"  \n",
    "        self.restore_best_vars(save = False)\n",
    "        \n",
    "        features_batch = np.tile(np.expand_dims(features, axis = 0), [self.num_bootstraps, 1, 1])\n",
    "        \n",
    "        PREDS = self.sess.run(self.sc_output, feed_dict = {self.features_pl: features_batch})\n",
    "        \n",
    "        preds_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            preds_list.append(PREDS[b,:,:])\n",
    "        \n",
    "        return preds_list\n",
    "   \n",
    "    def save(self, key = None):\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        if key:\n",
    "            self.saver.save(self.sess, self.checkpoint_dir + '/' + self.model_name + '-' + str(key))        \n",
    "\n",
    "    def restore_best_vars(self, save = True):\n",
    "        \"\"\"\n",
    "        Restores the variable values from the epoch in which each network achieved its lowest val loss\n",
    "        \"\"\"\n",
    "        # Assign the new vars which we have created to the variables in the original graph\n",
    "        all_assign_ops = [v.assign(self.best_vars[num]) for num, v in enumerate(self.trainable_vars)]\n",
    "        self.sess.run(all_assign_ops)\n",
    "        \n",
    "        if save:\n",
    "            # Save the new combined best variables and delete the null old checkpoints\n",
    "            self.save(key = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for running experiments using the bootstrap neural net - if just using 1 network, use original neural network, otherwise use new bootstrap NN from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time_network(bootstraps_list = [1, 2], iterations = 1):\n",
    "    \n",
    "    time_df = pd.DataFrame(columns = ['CPU', 'GPU'], index = bootstraps_list)\n",
    "    average_time_df = pd.DataFrame(columns = ['CPU', 'GPU'], index = bootstraps_list)\n",
    "    gpu_df = pd.DataFrame(columns = ['GPU % usage'], index = bootstraps_list)\n",
    "    \n",
    "   \n",
    "    for compute in ['CPU', 'GPU']:\n",
    "    \n",
    "        for b in bootstraps_list:\n",
    "            \n",
    "             # Put data in iterators \n",
    "            iter_dict = batch_sorter(list(housing_targets), list(housing_features), batch_size = 50)\n",
    "            bootstrap_iter_dict = bootstrap_batch_sorter(housing_targets, housing_features, batch_size = 50, num_bootstraps = b)\n",
    "  \n",
    "            times = []\n",
    "            gpu_usage = []\n",
    "\n",
    "            for i in range(iterations):\n",
    "\n",
    "                tf.reset_default_graph()\n",
    "\n",
    "                with tf.Session() as sess:\n",
    "\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    if compute == 'CPU':\n",
    "                        with tf.device('/cpu:0'):\n",
    "                            \n",
    "                            if b == 1:\n",
    "                                nn = NN(sess, iter_dict, num_layers = 3, num_hidden_nodes = 100,  learning_rate = 0.01, \n",
    "                                            target_scaling = True, feature_scaling = True, activation_fn = tf.nn.tanh)\n",
    "                            else:\n",
    "                                nn = bootstrap_NN(sess, bootstrap_iter_dict, num_layers = 3, num_hidden_nodes = 100,  learning_rate = 0.01, \n",
    "                                    target_scaling = True, feature_scaling = True, num_bootstraps = b, activation_fn = tf.nn.tanh)\n",
    "\n",
    "                            nn.train(num_steps = 3000, viz_every = 500)\n",
    "                            \n",
    "                            \n",
    "                    elif compute == 'GPU':\n",
    "                        \n",
    "                        if b == 1:\n",
    "                            nn = NN(sess, iter_dict, num_layers = 3, num_hidden_nodes = 100,  learning_rate = 0.01, \n",
    "                                            target_scaling = True, feature_scaling = True, activation_fn = tf.nn.tanh)\n",
    "                        else:\n",
    "                            nn = bootstrap_NN(sess, bootstrap_iter_dict, num_layers = 3, num_hidden_nodes = 100,  learning_rate = 0.01, \n",
    "                                    target_scaling = True, feature_scaling = True, num_bootstraps = b, activation_fn = tf.nn.tanh)\n",
    "                        \n",
    "                        gpu = monitor_gpu()\n",
    "                        gpu.start_monitoring()\n",
    "                        nn.train(num_steps = 3000, viz_every = 500)\n",
    "                        \n",
    "                display.clear_output()\n",
    "\n",
    "                times.append(time.time() - start_time)\n",
    "\n",
    "                if compute == 'GPU':\n",
    "                    gpu.stop_monitoring()\n",
    "                    gpu_usage.append(gpu.average_use)\n",
    "\n",
    "            time_df.ix[b,compute] = np.round(np.average(times),2)\n",
    "            average_time_df.ix[b,compute] = np.round(np.average(times)/float(b),2)\n",
    "            \n",
    "            if compute == 'GPU':\n",
    "                gpu_df.ix[b, 0] = np.round(np.average(gpu_usage), 2)\n",
    "                \n",
    "    return time_df, average_time_df, gpu_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiments - this will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 350 observations\n",
      "Val data: 100 observations\n",
      "Test data: 56 observations\n",
      "\n",
      "Train data: 350 observations\n",
      "Val data: 100 observations\n",
      "Test data: 56 observations\n",
      "\n",
      "Step: 0, Train Loss: 0.77, Val Loss: 0.87, Test Loss: 0.71\n",
      "Step: 500, Train Loss: 0.18, Val Loss: 0.25, Test Loss: 0.22\n",
      "Step: 1000, Train Loss: 0.11, Val Loss: 0.18, Test Loss: 0.17\n",
      "Step: 1500, Train Loss: 0.08, Val Loss: 0.18, Test Loss: 0.19\n",
      "Step: 2000, Train Loss: 0.07, Val Loss: 0.14, Test Loss: 0.15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-060f43a96a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_time_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstraps_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-42954e44519f>\u001b[0m in \u001b[0;36mtime_network\u001b[0;34m(bootstraps_list, iterations)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                     target_scaling = True, feature_scaling = True, num_bootstraps = b, activation_fn = tf.nn.tanh)\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/joseph/Desktop/GPU_Bootstrapping-master/blog_functions.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, viz_every, num_steps)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Initially just train the mean prediction network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mviz_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_df, average_time_df, gpu_df = time_network(bootstraps_list = [1, 2, 5, 10, 25, 50, 100, 150, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some test data to demonstrate effects of bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f21e26a6f90>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2210304990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_data(num_draws = 500):\n",
    "    \n",
    "    def curve(x):\n",
    "        return 1.5 * np.sin(3*x)  + x\n",
    "    \n",
    "    def noise(x):\n",
    "        return np.random.normal(loc = 0.0, scale = x * 0.1)\n",
    "    \n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    for _ in xrange(num_draws):\n",
    "        x_1 = np.random.uniform(low = 4.5, high = 6.0)\n",
    "        x_2 = np.random.uniform(low = 2.0, high = 3.5)\n",
    "        x = np.random.choice([x_1, x_2])\n",
    "        xs.append(x)\n",
    "        ys.append(curve(x) + noise(x))\n",
    "            \n",
    "    features = np.concatenate([np.expand_dims(f, axis = 0) for f in xs], axis = 0)\n",
    "    targets = np.concatenate([np.expand_dims(f, axis = 0) for f in ys], axis = 0)\n",
    "\n",
    "    return features, targets\n",
    "\n",
    "features, targets = test_data(num_draws = 500)\n",
    "\n",
    "plt.plot(features, targets, 'x', c = colours['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit 100 bootstrapped networks to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 350 observations\n",
      "Val data: 100 observations\n",
      "Test data: 50 observations\n",
      "\n",
      "Step: 0, Train Loss: 0.82, Val Loss: 0.82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-810c27b7b9c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     nn = bootstrap_NN(sess, iter_dict, num_layers = 3, num_hidden_nodes = 100,  learning_rate = 0.1, \n\u001b[1;32m     18\u001b[0m                 target_scaling = True, feature_scaling = True, num_bootstraps = num_bootstraps, activation_fn = tf.nn.tanh)\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d474e7cce9a4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, viz_every, num_steps)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# Initially just train the mean prediction network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             _ = self.sess.run(self.optimizer, feed_dict = {self.features_pl: f_batch, \n\u001b[0;32m--> 109\u001b[0;31m                                     self.targets_pl: t_batch})\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mviz_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpu = monitor_gpu()\n",
    "gpu.start_monitoring()\n",
    "\n",
    "num_bootstraps = 100\n",
    "\n",
    "# Put data in iterators \n",
    "iter_dict = bootstrap_batch_sorter(targets, features, batch_size = 50, num_bootstraps = num_bootstraps)\n",
    "\n",
    "x = np.expand_dims(np.linspace(-1, 9, 100), 1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the neural net to predict the mean and variance\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    nn = bootstrap_NN(sess, iter_dict, num_layers = 3, num_hidden_nodes = 100,  learning_rate = 0.1, \n",
    "                target_scaling = True, feature_scaling = True, num_bootstraps = num_bootstraps, activation_fn = tf.nn.tanh)\n",
    "    nn.train(num_steps = 5000, viz_every = 500)\n",
    "\n",
    "    preds = nn.predict(x)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print \"\\nTime: {:.2f} seconds\".format(total_time)\n",
    "\n",
    "gpu.stop_monitoring()\n",
    "\n",
    "print '\\nGPU average usage: {:.2f}%'.format(gpu.average_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-00868ad9c39f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolours\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21d8bb0e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = single_blog_graph()\n",
    "\n",
    "ax.plot(features, targets, 'x', c = colours['blue'])\n",
    "\n",
    "iterator = itertools.cycle(colours.values()[:-1])\n",
    "\n",
    "for p in preds:\n",
    "    c = iterator.next()\n",
    "    ax.plot(x, p, c = c)\n",
    "    \n",
    "_ = ax.set_xlim([0, 8])\n",
    "    \n",
    "#fig.savefig('Pictures/gpu_bootstrap/bootstrap.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An adaptation of the bootstrap NN above to allow for different learning rates for each parameter. This allows rapid experimentation to find the optimum learning rate on a GPU. At the moment just applies for simple gradient descent optimization - would need some thought for more complex optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class learning_rate_NN(object):\n",
    "    \n",
    "    def __init__(self, sess, batch_iterators, num_layers = 1, num_hidden_nodes = 30, activation_fn = tf.nn.relu,\n",
    "                 learning_rate = 0.00001, ind_learning_rates = [0.00001] * 10, model_name = 'NN', \n",
    "                 target_scaling = True, feature_scaling = True, checkpoint_dir = 'checkpoint_bootstrap', \n",
    "                 num_bootstraps= 10):\n",
    "        \n",
    "        assert len(ind_learning_rates) == num_bootstraps, 'Need to have separate learning rate for each bootstrap'\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.train_iter = batch_iterators['train']\n",
    "        self.val_iter = batch_iterators['val']\n",
    "        self.test_iter = batch_iterators['test']\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.learning_rate = learning_rate * num_bootstraps\n",
    "        self.ind_learning_rates = ind_learning_rates\n",
    "                \n",
    "        self.targets_dim = self.train_iter.targets_dim\n",
    "        self.features_dim = self.train_iter.features_dim\n",
    "        \n",
    "        self.target_scaling = target_scaling\n",
    "        self.feature_scaling = feature_scaling\n",
    "                \n",
    "        self.num_bootstraps = num_bootstraps\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        self.best_step = [0] * self.num_bootstraps\n",
    "        self.best_val_losses = [float('inf')] * self.num_bootstraps\n",
    "    \n",
    "         # Scalers\n",
    "        self.t_scaler, self.rev_t_scaler = add_scaler(self.train_iter.target_mean, self.train_iter.target_std,\n",
    "                                                      scaling = self.target_scaling, name = 'targets')\n",
    "        self.f_scaler, _ = add_scaler(self.train_iter.feature_mean, self.train_iter.feature_std, \n",
    "                                      scaling = self.feature_scaling, name = 'features')\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        self.saver = tf.train.Saver(max_to_keep = None)\n",
    "        \n",
    "       \n",
    "    def build_model(self):\n",
    "        self.targets_pl = tf.placeholder(tf.float32, [self.num_bootstraps, None, self.targets_dim], 'targets_pl')\n",
    "        self.features_pl = tf.placeholder(tf.float32, [self.num_bootstraps, None, self.features_dim], 'features_pl')\n",
    "        \n",
    "        # Scaling step\n",
    "        self.targets = self.t_scaler(self.targets_pl)\n",
    "        self.features = self.f_scaler(self.features_pl)        \n",
    "\n",
    "        layer_list = []\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            \n",
    "            if layer == 0:\n",
    "                input_size = self.features_dim\n",
    "                input_matrix = self.features\n",
    "            else:\n",
    "                input_size = self.num_hidden_nodes\n",
    "                input_matrix = layer_list[layer - 1]\n",
    "            \n",
    "            weights = tf.Variable(tf.truncated_normal([self.num_bootstraps, input_size, self.num_hidden_nodes], stddev = 0.1), name = 'weights_layer_' + str(layer))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape = [self.num_bootstraps, 1, self.num_hidden_nodes]), name = 'bias_layer_' + str(layer))\n",
    "                        \n",
    "            layer_inner = tf.matmul(input_matrix, weights) + bias\n",
    "                    \n",
    "            layer_list.append(self.activation_fn(layer_inner))\n",
    "\n",
    "        # Output layer and losses\n",
    "                \n",
    "        output_weights = tf.Variable(tf.truncated_normal([self.num_bootstraps, self.num_hidden_nodes, self.targets_dim], stddev = 0.1), name = 'weights_final_layer')\n",
    "        output_bias = tf.Variable(tf.constant(0.1, shape = [self.num_bootstraps, 1, self.targets_dim]), name = 'bias_final_layer')\n",
    "                              \n",
    "        self.output = tf.matmul(layer_list[self.num_layers - 1], output_weights) + output_bias\n",
    "        self.sc_output = self.rev_t_scaler(self.output)\n",
    "                    \n",
    "        # Create separate loss functions - these will not be used for training as faster with one loss op, but will\n",
    "        # be used at eval stage\n",
    "        \n",
    "        self.loss_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            self.loss_list.append(tf.reduce_mean(tf.pow(self.targets[b,:,:] - self.output[b,:,:], 2), name = 'loss_' + str(b)))\n",
    "        \n",
    "        # Main loss op used for training\n",
    "        self.loss = tf.reduce_mean(tf.pow(self.targets - self.output, 2), name = 'loss')\n",
    "        \n",
    "        # Create constant to multiply the gradients by in order to apply different learning rates\n",
    "        np_lr_factors = np.expand_dims(np.expand_dims(self.ind_learning_rates/float(self.learning_rate/float(self.num_bootstraps)),1),2)\n",
    "        tf_lr_factors = tf.constant(np_lr_factors, dtype = tf.float32) # shape will be [num_bootstraps * 1 * 1]\n",
    "        \n",
    "        # Optimizers\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss)\n",
    "        \n",
    "        # Scale the gradients by the separate learning rate factors\n",
    "        new_grads_and_vars =[(g*tf_lr_factors, v) for g, v in grads_and_vars]\n",
    "        self.optimizer = self.opt.apply_gradients(new_grads_and_vars)\n",
    "        \n",
    "        # Trainable vars\n",
    "        self.trainable_vars = tf.trainable_variables()\n",
    "\n",
    "    def train(self, viz_every = 500, num_steps = 5000):\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "        for step in xrange(num_steps):\n",
    "            \n",
    "            t_batch, f_batch = self.train_iter.next_batch()\n",
    "            \n",
    "            # Initially just train the mean prediction network \n",
    "            _ = self.sess.run(self.optimizer, feed_dict = {self.features_pl: f_batch, \n",
    "                                    self.targets_pl: t_batch})\n",
    "\n",
    "            if step % viz_every == 0:\n",
    "                \n",
    "                _, TRAIN_LOSSES = self.run_data_set(self.train_iter)\n",
    "                _, VAL_LOSSES = self.run_data_set(self.train_iter)\n",
    "                                \n",
    "                print \"Step: {0}, Train Loss: {1:.2f}, Val Loss: {2:.2f}\".format(step,\n",
    "                                        np.average(TRAIN_LOSSES), np.average(VAL_LOSSES))    \n",
    "                \n",
    "                save_update = False\n",
    "                \n",
    "                # Update the best val losses\n",
    "                for b in range(self.num_bootstraps):\n",
    "                    if VAL_LOSSES[b] < self.best_val_losses[b]:\n",
    "                        self.best_val_losses[b] = VAL_LOSSES[b]\n",
    "                        self.best_step[b] = step\n",
    "                        save_update = True\n",
    "                \n",
    "                # Save if any of the separate bootstrapped networks have improved on their best val loss\n",
    "                if save_update:                    \n",
    "                    T_VARS = self.sess.run(self.trainable_vars)\n",
    "                    self.update_t_vars(T_VARS, step = step)\n",
    "                    \n",
    "        self.restore_best_vars()\n",
    "                \n",
    "        self.TRAIN_PREDS, self.TRAIN_LOSSES = self.run_data_set(self.train_iter)\n",
    "        self.VAL_PREDS, self.VAL_LOSSES = self.run_data_set(self.train_iter)\n",
    "        self.TEST_PREDS, self.TEST_LOSSES = self.run_data_set(self.test_iter)\n",
    "                \n",
    "        print \"Final Losses, Train: {1:.2f}, Val: {2:.2f}, Test: {3:.2f}\".format(step,\n",
    "                                            np.average(self.TRAIN_LOSSES), np.average(self.VAL_LOSSES), np.average(self.TEST_LOSSES)) \n",
    "        \n",
    "                    \n",
    "    \n",
    "    def run_data_set(self, iterator):\n",
    "        \n",
    "        # Store starting value of iterator to return to\n",
    "        counter_start = iterator.counter\n",
    "        # Make sure we start from the first batch\n",
    "        iterator.counter = 0\n",
    "        \n",
    "        preds_dict = {}\n",
    "        loss_dict = {}\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            preds_dict[b] = []\n",
    "            loss_dict[b] = []\n",
    "        \n",
    "        for step in xrange(iterator.num_batches):\n",
    "            \n",
    "            t_batch, f_batch = iterator.next_batch()\n",
    "            \n",
    "            OUTPUT = self.sess.run([self.sc_output] + self.loss_list, feed_dict = {self.features_pl: f_batch, \n",
    "                                                    self.targets_pl: t_batch})\n",
    "                 \n",
    "            LOSSES = OUTPUT[1:]\n",
    "            PREDS = OUTPUT[0]\n",
    "            \n",
    "            for b in range(self.num_bootstraps):\n",
    "                preds_dict[b].append(PREDS[b, :, :])\n",
    "                loss_dict[b].append(LOSSES[b])\n",
    "                \n",
    "        all_preds_list = []\n",
    "        average_loss_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            all_preds_list.append(np.concatenate(preds_dict[b], axis = 0))\n",
    "            average_loss_list.append(np.average(loss_dict[b]))\n",
    "        # Return iterator counter to starting value\n",
    "        iterator.counter = counter_start\n",
    "        \n",
    "        return all_preds_list, average_loss_list          \n",
    "        \n",
    "\n",
    "    \n",
    "    def update_t_vars(self, current_vars, step):\n",
    "        \n",
    "        if not hasattr(self, 'best_vars'):            \n",
    "            self.best_vars = [np.zeros_like(v) for v in current_vars]\n",
    "        \n",
    "        # If loss was lowest for this bootstrap with these variables, update our new best variables\n",
    "        for b in range(self.num_bootstraps):\n",
    "            if self.best_step[b] == step:\n",
    "                # Assign the correct parts of these variables to our new var list\n",
    "                for num, v in enumerate(self.best_vars):\n",
    "                    v[b] = current_vars[num][b]      \n",
    "    \n",
    "    def predict(self, features):\n",
    "        \n",
    "        self.restore_best_vars(save = False)\n",
    "        \n",
    "        features_batch = np.tile(np.expand_dims(features, axis = 0), [self.num_bootstraps, 1, 1])\n",
    "        \n",
    "        PREDS = self.sess.run(self.sc_output, feed_dict = {self.features_pl: features_batch})\n",
    "        \n",
    "        preds_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            preds_list.append(PREDS[b,:,:])\n",
    "        \n",
    "        return preds_list\n",
    "   \n",
    "    def save(self, key = None):\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        if key:\n",
    "            self.saver.save(self.sess, self.checkpoint_dir + '/' + self.model_name + '-' + str(key))        \n",
    "\n",
    "    def restore_best_vars(self, save = True):\n",
    "        # Assign the new vars which we have created to the variables in the original graph\n",
    "        all_assign_ops = [v.assign(self.best_vars[num]) for num, v in enumerate(self.trainable_vars)]\n",
    "        self.sess.run(all_assign_ops)\n",
    "        \n",
    "        if save:\n",
    "            # Save the new combined best variables and delete the null old checkpoints\n",
    "            self.save(key = 'best')\n",
    "            \n",
    "    def best_learning_rate(self):\n",
    "        ix = np.argmin(self.VAL_LOSSES)\n",
    "        best_learning_rate = self.ind_learning_rates[ix]        \n",
    "        print \"\\nBest learning rate: {:.3f}, train loss: {:.2f}, val loss: {:.2f}, test loss: {:.2f}\".format(best_learning_rate,\n",
    "                                                                                self.TRAIN_LOSSES[ix], self.VAL_LOSSES[ix], self.TEST_LOSSES[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with 100 different learning rates on the housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 350 observations\n",
      "Val data: 100 observations\n",
      "Test data: 56 observations\n",
      "\n",
      "Step: 0, Train Loss: 0.87, Val Loss: 0.87\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7afdbfe669f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     nn = learning_rate_NN(sess, iter_dict, num_layers = 2, num_hidden_nodes = 20, ind_learning_rates = ind_learning_rates,\n\u001b[1;32m     18\u001b[0m                 target_scaling = True, feature_scaling = True, num_bootstraps = num_bootstraps, activation_fn = tf.nn.tanh)\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9be1f39b6fb8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, viz_every, num_steps)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# Initially just train the mean prediction network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             _ = self.sess.run(self.optimizer, feed_dict = {self.features_pl: f_batch, \n\u001b[0;32m--> 121\u001b[0;31m                                     self.targets_pl: t_batch})\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mviz_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpu = monitor_gpu()\n",
    "gpu.start_monitoring()\n",
    "\n",
    "num_bootstraps = 100\n",
    "\n",
    "# Put data in iterators \n",
    "iter_dict = bootstrap_batch_sorter(housing_targets, housing_features, batch_size = 50, num_bootstraps = num_bootstraps)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ind_learning_rates = np.linspace(0.000001, 0.5, num_bootstraps)\n",
    "\n",
    "# Run the neural net to predict the mean and variance\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    nn = learning_rate_NN(sess, iter_dict, num_layers = 2, num_hidden_nodes = 20, ind_learning_rates = ind_learning_rates,\n",
    "                target_scaling = True, feature_scaling = True, num_bootstraps = num_bootstraps, activation_fn = tf.nn.tanh)\n",
    "    nn.train(num_steps = 3000, viz_every = 500)\n",
    "    nn.best_learning_rate()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print \"\\nTime: {:.2f} seconds\".format(total_time)\n",
    "\n",
    "gpu.stop_monitoring()\n",
    "\n",
    "print '\\nGPU average usage: {:.2f}%'.format(gpu.average_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the val losses for each learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAESCAYAAAAMifkAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95926ea990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = single_blog_graph()\n",
    "\n",
    "ax.plot(nn.ind_learning_rates, nn.VAL_LOSSES, c = colours['green'], linewidth = 2.0)\n",
    "ax.set_xlabel('Learning Rate')\n",
    "_ = ax.set_ylabel('Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attempt at providing a version which works with multiple different layer sizes - works mathematically, but doesn't utilize the GPU effectively in current form, so much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class variable_size_NN(object):\n",
    "    \n",
    "    def __init__(self, sess, batch_iterators, num_hidden_nodes, activation_fn = tf.nn.relu,\n",
    "                 learning_rate = 0.00001, model_name = 'NN', target_scaling = True, feature_scaling = True,\n",
    "                 checkpoint_dir = 'checkpoint_bootstrap', num_bootstraps= 10):\n",
    "        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.train_iter = batch_iterators['train']\n",
    "        self.val_iter = batch_iterators['val']\n",
    "        self.test_iter = batch_iterators['test']\n",
    "        \n",
    "        self.num_layers = num_hidden_nodes.shape[1]\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.learning_rate = learning_rate * num_bootstraps\n",
    "                \n",
    "        self.targets_dim = self.train_iter.targets_dim\n",
    "        self.features_dim = self.train_iter.features_dim\n",
    "        \n",
    "        self.target_scaling = target_scaling\n",
    "        self.feature_scaling = feature_scaling\n",
    "                \n",
    "        self.num_bootstraps = num_bootstraps\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        self.best_step = [0] * self.num_bootstraps\n",
    "        self.best_val_losses = [float('inf')] * self.num_bootstraps\n",
    "    \n",
    "        \n",
    "         # Scalers\n",
    "        self.t_scaler, self.rev_t_scaler = add_scaler(self.train_iter.target_mean, self.train_iter.target_std,\n",
    "                                                      scaling = self.target_scaling, name = 'targets')\n",
    "        self.f_scaler, _ = add_scaler(self.train_iter.feature_mean, self.train_iter.feature_std, \n",
    "                                      scaling = self.feature_scaling, name = 'features')\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        self.saver = tf.train.Saver(max_to_keep = None)\n",
    "        \n",
    "       \n",
    "    def build_model(self):\n",
    "        self.targets_pl = tf.placeholder(tf.float32, [self.num_bootstraps, None, self.targets_dim], 'targets_pl')\n",
    "        self.features_pl = tf.placeholder(tf.float32, [self.num_bootstraps, None, self.features_dim], 'features_pl')\n",
    "        \n",
    "        # Scaling step\n",
    "        self.targets = self.t_scaler(self.targets_pl)\n",
    "        self.features = self.f_scaler(self.features_pl)        \n",
    "\n",
    "        layer_list = []\n",
    "        self.var_list = []\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            \n",
    "            if layer == 0:\n",
    "                input_size = np.tile(self.features_dim, [self.num_bootstraps])\n",
    "                input_matrix = self.features\n",
    "                max_previous = self.features_dim\n",
    "            else:\n",
    "                input_size = self.num_hidden_nodes[:,layer-1]\n",
    "                input_matrix = layer_list[layer - 1]\n",
    "                max_previous = np.max(input_size)\n",
    "            \n",
    "            weights_list = []\n",
    "            bias_list = []\n",
    "            max_current = np.max(self.num_hidden_nodes[:,layer])\n",
    "            \n",
    "            for b in range(self.num_bootstraps):\n",
    "                trainable_weights = tf.Variable(tf.truncated_normal([1, input_size[b], self.num_hidden_nodes[b,layer]], stddev = 0.1), \n",
    "                                                name = 'weights_layer_' + str(layer) + '_' + str(b))\n",
    "                padding_current = tf.Variable(tf.constant(0.0, shape = [1, input_size[b], max_current - self.num_hidden_nodes[b,layer]]), trainable = False)\n",
    "                padding_previous = tf.Variable(tf.constant(0.0, shape = [1, max_previous - input_size[b], max_current]), trainable = False)\n",
    "                \n",
    "                joined = tf.concat([trainable_weights, padding_current], axis = 2)\n",
    "                joined_2 = tf.concat([joined, padding_previous], axis = 1)\n",
    "                weights_list.append(joined_2)\n",
    "                \n",
    "                bias_trainable = tf.Variable(tf.constant(0.1, shape = [1, 1, self.num_hidden_nodes[b,layer]]), name = 'bias_layer_' + str(layer) +'_' + str(b))\n",
    "                bias_padding = tf.Variable(tf.constant(0.0, shape = [1,1,max_current - self.num_hidden_nodes[b,layer]]), trainable = False)\n",
    "                bias_list.append(tf.concat([bias_trainable, bias_padding], axis = 2))\n",
    "                \n",
    "            weights = tf.concat(weights_list, axis = 0)\n",
    "            bias = tf.concat(bias_list, axis = 0)\n",
    "            \n",
    "            self.var_list.append(weights)\n",
    "            self.var_list.append(bias)\n",
    "            \n",
    "            layer_inner = tf.matmul(input_matrix, weights) + bias\n",
    "                    \n",
    "            layer_list.append(self.activation_fn(layer_inner))\n",
    "\n",
    "        # Output layer and losses\n",
    "        out_weights = []\n",
    "        out_bias = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            input_size = self.num_hidden_nodes[:,-1]\n",
    "            max_previous = np.max(input_size)\n",
    "            trainable_out_weights = tf.Variable(tf.truncated_normal([1, input_size[b], self.targets_dim], stddev = 0.1), \n",
    "                                         name = 'weights_final_layer_' + str(b))\n",
    "            output_padding = tf.Variable(tf.constant(0.0, shape = [1, max_previous - input_size[b], self.targets_dim]), trainable = False)\n",
    "            out_weights.append(tf.concat([trainable_out_weights, output_padding], axis = 1))\n",
    "            \n",
    "        output_bias = tf.Variable(tf.constant(0.1, shape = [self.num_bootstraps,1, self.targets_dim]), name = 'bias_final_layer')          \n",
    "        output_weights = tf.concat(out_weights, axis = 0)\n",
    "        \n",
    "        self.var_list.append(output_weights)\n",
    "        self.var_list.append(output_bias)\n",
    "        \n",
    "        self.output = tf.matmul(layer_list[self.num_layers - 1], output_weights) + output_bias\n",
    "        self.sc_output = self.rev_t_scaler(self.output)\n",
    "                    \n",
    "        # Create separate loss functions - these will not be used for training as faster with one loss op, but will\n",
    "        # be used at eval stage\n",
    "        \n",
    "        self.loss_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            self.loss_list.append(tf.reduce_mean(tf.pow(self.targets[b,:,:] - self.output[b,:,:], 2), name = 'loss_' + str(b)))\n",
    "        \n",
    "        # Main loss op used for training\n",
    "        self.loss = tf.reduce_mean(tf.pow(self.targets - self.output, 2), name = 'loss')\n",
    "                       \n",
    "        # Optimizers\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def train(self, viz_every = 500, num_steps = 5000):\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "        for step in xrange(num_steps):\n",
    "            \n",
    "            t_batch, f_batch = self.train_iter.next_batch()\n",
    "            \n",
    "            # Initially just train the mean prediction network \n",
    "            _ = self.sess.run(self.optimizer, feed_dict = {self.features_pl: f_batch, \n",
    "                                    self.targets_pl: t_batch})\n",
    "\n",
    "            if step % viz_every == 0:\n",
    "                \n",
    "                _, TRAIN_LOSSES = self.run_data_set(self.train_iter)\n",
    "                _, VAL_LOSSES = self.run_data_set(self.train_iter)\n",
    "                                \n",
    "                print \"Step: {0}, Train Loss: {1:.2f}, Val Loss: {2:.2f}\".format(step,\n",
    "                                        np.average(TRAIN_LOSSES), np.average(VAL_LOSSES))    \n",
    "                \n",
    "                save_update = False\n",
    "                \n",
    "                # Update the best val losses\n",
    "                for b in range(self.num_bootstraps):\n",
    "                    if VAL_LOSSES[b] < self.best_val_losses[b]:\n",
    "                        self.best_val_losses[b] = VAL_LOSSES[b]\n",
    "                        self.best_step[b] = step\n",
    "                        save_update = True\n",
    "                \n",
    "                # Save if any of the separate bootstrapped networks have improved on their best val loss\n",
    "                if save_update:                    \n",
    "                    T_VARS = self.sess.run(self.var_list)\n",
    "                    self.update_t_vars(T_VARS, step = step)\n",
    "                    \n",
    "        #self.restore_best_vars()\n",
    "                \n",
    "        self.TRAIN_PREDS, self.TRAIN_LOSSES = self.run_data_set(self.train_iter)\n",
    "        self.VAL_PREDS, self.VAL_LOSSES = self.run_data_set(self.train_iter)\n",
    "        self.TEST_PREDS, self.TEST_LOSSES = self.run_data_set(self.test_iter)\n",
    "                \n",
    "        print \"Final Losses, Train: {1:.2f}, Val: {2:.2f}, Test: {3:.2f}\".format(step,\n",
    "                                            np.average(self.TRAIN_LOSSES), np.average(self.VAL_LOSSES), np.average(self.TEST_LOSSES)) \n",
    "        \n",
    "                    \n",
    "    \n",
    "    def run_data_set(self, iterator):\n",
    "        \n",
    "        # Store starting value of iterator to return to\n",
    "        counter_start = iterator.counter\n",
    "        # Make sure we start from the first batch\n",
    "        iterator.counter = 0\n",
    "        \n",
    "        preds_dict = {}\n",
    "        loss_dict = {}\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            preds_dict[b] = []\n",
    "            loss_dict[b] = []\n",
    "        \n",
    "        for step in xrange(iterator.num_batches):\n",
    "            \n",
    "            t_batch, f_batch = iterator.next_batch()\n",
    "            \n",
    "            OUTPUT = self.sess.run([self.sc_output] + self.loss_list, feed_dict = {self.features_pl: f_batch, \n",
    "                                                    self.targets_pl: t_batch})\n",
    "                 \n",
    "            LOSSES = OUTPUT[1:]\n",
    "            PREDS = OUTPUT[0]\n",
    "            \n",
    "            for b in range(self.num_bootstraps):\n",
    "                preds_dict[b].append(PREDS[b, :, :])\n",
    "                loss_dict[b].append(LOSSES[b])\n",
    "                \n",
    "        all_preds_list = []\n",
    "        average_loss_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            all_preds_list.append(np.concatenate(preds_dict[b], axis = 0))\n",
    "            average_loss_list.append(np.average(loss_dict[b]))\n",
    "        # Return iterator counter to starting value\n",
    "        iterator.counter = counter_start\n",
    "        \n",
    "        return all_preds_list, average_loss_list          \n",
    "        \n",
    "\n",
    "    \n",
    "    def update_t_vars(self, current_vars, step):\n",
    "        \n",
    "        if not hasattr(self, 'best_vars'):            \n",
    "            self.best_vars = [np.zeros_like(v) for v in current_vars]\n",
    "        \n",
    "        # If loss was lowest for this bootstrap with these variables, update our new best variables\n",
    "        for b in range(self.num_bootstraps):\n",
    "            if self.best_step[b] == step:\n",
    "                # Assign the correct parts of these variables to our new var list\n",
    "                for num, v in enumerate(self.best_vars):\n",
    "                    v[b] = current_vars[num][b]      \n",
    "    \n",
    "    def predict(self, features):\n",
    "        \n",
    "        #self.restore_best_vars(save = False)\n",
    "        \n",
    "        features_batch = np.tile(np.expand_dims(features, axis = 0), [self.num_bootstraps, 1, 1])\n",
    "        \n",
    "        PREDS = self.sess.run(self.sc_output, feed_dict = {self.features_pl: features_batch})\n",
    "        \n",
    "        preds_list = []\n",
    "        \n",
    "        for b in range(self.num_bootstraps):\n",
    "            preds_list.append(PREDS[b,:,:])\n",
    "        \n",
    "        return preds_list\n",
    "   \n",
    "    def save(self, key = None):\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        if key:\n",
    "            self.saver.save(self.sess, self.checkpoint_dir + '/' + self.model_name + '-' + str(key))        \n",
    "\n",
    "    def restore_best_vars(self, save = True):\n",
    "        # Assign the new vars which we have created to the variables in the original graph\n",
    "        all_assign_ops = [v.assign(self.best_vars[num]) for num, v in enumerate(self.var_list)]\n",
    "        self.sess.run(all_assign_ops)\n",
    "        \n",
    "        if save:\n",
    "            # Save the new combined best variables and delete the null old checkpoints\n",
    "            self.save(key = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the variable sizes network - at the moment this doesn't utilize the GPU effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 350 observations\n",
      "Val data: 100 observations\n",
      "Test data: 50 observations\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fa68ae97725a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     nn = variable_size_NN(sess, iter_dict, num_hidden_nodes = hidden_nodes, learning_rate = 0.1, \n\u001b[0;32m---> 19\u001b[0;31m                 target_scaling = True, feature_scaling = True, num_bootstraps = hidden_nodes.shape[0], activation_fn = tf.nn.tanh)\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2f7ce4252456>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, batch_iterators, num_hidden_nodes, activation_fn, learning_rate, model_name, target_scaling, feature_scaling, checkpoint_dir, num_bootstraps)\u001b[0m\n\u001b[1;32m     37\u001b[0m                                       scaling = self.feature_scaling, name = 'features')\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2f7ce4252456>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 trainable_weights = tf.Variable(tf.truncated_normal([1, input_size[b], self.num_hidden_nodes[b,layer]], stddev = 0.1), \n\u001b[1;32m     72\u001b[0m                                                 name = 'weights_layer_' + str(layer) + '_' + str(b))\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mpadding_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_current\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0mpadding_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_previous\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_current\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    101\u001b[0m   const_tensor = g.create_op(\n\u001b[1;32m    102\u001b[0m       \u001b[0;34m\"Const\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m       attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n\u001b[0m\u001b[1;32m    104\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;31m# calls the C / C-API directly, we should be able to remove this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     return {\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;34m\"shapes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;34m\"handle_shapes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;34m\"handle_dtypes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhichOneof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpu = monitor_gpu()\n",
    "gpu.start_monitoring()\n",
    "\n",
    "layer_sizes = [[10,20,30,40,50],[10,20,30,40],[10,20,30,40]]\n",
    "hidden_nodes = np.array(list(itertools.product(*layer_sizes)))\n",
    "\n",
    "# Put data in iterators \n",
    "iter_dict = bootstrap_batch_sorter(targets, features, batch_size = 50, num_bootstraps = hidden_nodes.shape[0])\n",
    "\n",
    "x = np.expand_dims(np.linspace(-1, 9, 100), 1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the neural net to predict the mean and variance\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    nn = variable_size_NN(sess, iter_dict, num_hidden_nodes = hidden_nodes, learning_rate = 0.1, \n",
    "                target_scaling = True, feature_scaling = True, num_bootstraps = hidden_nodes.shape[0], activation_fn = tf.nn.tanh)\n",
    "    nn.train(num_steps = 2000, viz_every = 500)\n",
    "\n",
    "    preds = nn.predict(x)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print \"\\nTime: {:.2f} seconds\".format(total_time)\n",
    "\n",
    "gpu.stop_monitoring()\n",
    "\n",
    "print '\\nGPU average usage: {:.2f}%'.format(gpu.average_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
